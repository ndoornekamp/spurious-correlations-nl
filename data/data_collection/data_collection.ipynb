{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a76f044c",
   "metadata": {},
   "source": [
    "# IMDb data\n",
    "Number of movies that featured an actor/actress. IMDb publishes their data on https://www.imdb.com/interfaces/, so no need for webscraping. I used the `imdb-sqlite` package to parse IMDb's data into an SQlite database, which can be queried easily using `sqlalchemy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import inspect\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('sqlite:///imdb.db')\n",
    "engine.connect()\n",
    "\n",
    "inspector = inspect(engine)\n",
    "table_names = inspector.get_table_names()\n",
    "\n",
    "for table_name in table_names:\n",
    "    print(f\"Table:{table_name}\")\n",
    "    column_items = inspector.get_columns(table_name)\n",
    "    print('\\t'.join(n for n in column_items[0]))\n",
    "    for c in column_items:\n",
    "        assert len(c) == len(column_items[0])\n",
    "        print('\\t'.join(str(c[n]) for n in c))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475dcd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the classes SQLalchemy needs to work with the SQlite database\n",
    "\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy import Column, String, Integer\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class People(Base):\n",
    "    __tablename__ = \"people\"\n",
    "    \n",
    "    person_id = Column(String, primary_key=True)\n",
    "    name = Column(String)\n",
    "    born = Column(Integer)\n",
    "    died = Column(Integer)\n",
    "\n",
    "class Titles(Base):\n",
    "    __tablename__ = 'titles'\n",
    "    \n",
    "    title_id = Column(String, primary_key=True)\n",
    "    type = Column(String)\n",
    "    primary_title = Column(String)\n",
    "    original_title = Column(String)\n",
    "    is_adult = Column(Integer)\n",
    "    premiered = Column(Integer)\n",
    "    ended = Column(Integer)\n",
    "    runtime_minutes = Column(Integer)\n",
    "    genres = Column(String)\n",
    "    \n",
    "\n",
    "class Crew(Base):\n",
    "    __tablename__ = 'crew'\n",
    "    \n",
    "    title_id = Column(String, primary_key=True)\n",
    "    person_id = Column(String)\n",
    "    category = Column(String)\n",
    "    job = Column(String)\n",
    "    characters = Column(String)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.title_id} - {self.person_id} - {self.category} - {self.job} - {self.characters}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d7113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "actors = [\n",
    "    \"Brad Pitt\",\n",
    "    \"Angelina Jolie\",\n",
    "    \"Johnny Depp\",\n",
    "    \"Nicolas Cage\",\n",
    "    \"Morgan Freeman\",\n",
    "    \"Samuel L. Jackson\",\n",
    "    \"Benedict Cumberbatch\",\n",
    "    \"Robert Downey Jr.\",\n",
    "    \"Ryan Gosling\",\n",
    "    \"Liam Neeson\",\n",
    "    \"Will Smith\",\n",
    "    \"Leonardo DiCaprio\",\n",
    "    \"Tom Cruise\",\n",
    "    \"Scarlett Johansson\",\n",
    "    \"Will Smith\",\n",
    "    \"Jennifer Aniston\",\n",
    "    \"Julia Roberts\",\n",
    "]\n",
    "\n",
    "per_actor = {}\n",
    "\n",
    "for actor in tqdm(actors):\n",
    "    result = session.query(Crew, Titles.primary_title, Titles.premiered, People.name).join(Titles, Titles.title_id==Crew.title_id).join(People, People.person_id==Crew.person_id).filter(People.name==actor, Titles.type==\"movie\").all()\n",
    "    years = [title.premiered for title in result]\n",
    "    per_actor[actor] = {\n",
    "        \"movies_per_year\": dict(Counter(years)),\n",
    "        \"total\": len(years)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c38214",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2014, 2021))\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d30ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_time_series = []\n",
    "\n",
    "for actor in per_actor:\n",
    "    imdb_time_series.append({\n",
    "        \"title\": {\n",
    "            \"en\": f\"Number of movies featuring {actor}\",\n",
    "            \"nl\": f\"Aantal films met {actor}\"\n",
    "        },\n",
    "        \"axis_label\": {\n",
    "            \"en\": \"Number of movies\",\n",
    "            \"nl\": \"Aantal films\"\n",
    "        },\n",
    "        \"source\": \"https://www.imdb.com/interfaces/\",\n",
    "        \"values\": [per_actor[actor]['movies_per_year'].get(year, 0) for year in years]\n",
    "    }) \n",
    "    \n",
    "imdb_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46fd1ea",
   "metadata": {},
   "source": [
    "# KNMI data\n",
    "Annual reviews of the KNMI (Dutch meteorology institute), counting the number of days in certain categories. The tables on their website can be scraped effortlessly straight into a Pandas dataframe using read_html()-functionality, which works great for this website and in one line of code returns a DataFrame containing the data we wanted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd19cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "days = {}\n",
    "\n",
    "for year in years:\n",
    "    table = pd.read_html(f'https://www.knmi.nl/nederland-nu/klimatologie/maand-en-seizoensoverzichten/{year}/jaar', flavor='html5lib')\n",
    "    \n",
    "    table = table[0].replace('zomers dagen', 'zomerse dagen')\n",
    "    \n",
    "    try:\n",
    "        days[year] = dict(zip(table[2][1:], table[0][1:]))\n",
    "    except KeyError:\n",
    "        days[year] = dict(zip(table.iloc[:, 2][1:], table.iloc[:, 0][1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8fb84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "types_of_day = {\n",
    "    'Tropische dagen': 'tropical days (>25 degrees Celcius)',\n",
    "    'Zomerse dagen': 'summer days (>25 degrees Celcius)',\n",
    "    'Warme dagen': 'warm days (>20 degrees Celcius)',\n",
    "    'Vorstdagen': 'freezing days (<0 degrees Celcius)'\n",
    "}\n",
    "\n",
    "climate_time_series = []\n",
    "for type_of_day in types_of_day.keys():\n",
    "    values = []\n",
    "    for year in years:\n",
    "        if type_of_day in days[year]:\n",
    "            values.append(int(days[year][type_of_day]))\n",
    "        else:\n",
    "            values.append(int(days[year][type_of_day.lower()]))\n",
    "            \n",
    "    climate_time_series.append({\n",
    "        \"title\": {\n",
    "            \"en\": f\"Number of {types_of_day[type_of_day]} in de Bilt\",\n",
    "            \"nl\": f\"Aantal {type_of_day.lower()} in de Bilt\"\n",
    "        },\n",
    "        \"axis_label\": {\n",
    "            \"en\": \"Number of days\",\n",
    "            \"nl\": \"Aantal dagen\"\n",
    "        },\n",
    "        \"source\": \"https://www.knmi.nl/nederland-nu/klimatologie/maand-en-seizoensoverzichten/\",\n",
    "        \"values\": values\n",
    "    })\n",
    "\n",
    "climate_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571cf69e",
   "metadata": {},
   "source": [
    "# Names of newborns\n",
    "The SVB (Dutch social security bank) publishes yearly statistics on the number of names given to newborn babies. Only names occuring >25 times are included. The data is published in non-webscraper-friendly tables on their website, but those tables are fed by JSON textfiles which can be accessed as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de2a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "boy_names_urls = {\n",
    "    \"https://www.svbkindernamen.nl/nl/media/jongens-2016.txt\": 2016,\n",
    "    \"https://www.svbkindernamen.nl/nl/media/jongens-2017.txt\": 2017,\n",
    "    \"https://www.svbkindernamen.nl/nl/media/jongens-2018.txt\": 2018,\n",
    "    \"https://www.svbkindernamen.nl/nl/media/jongens-2019.txt\": 2019,\n",
    "    \"https://www.svbkindernamen.nl/nl/media/jongensnamen-2020-25-populair-json.txt\": 2020,\n",
    "}\n",
    "\n",
    "girl_names_urls = {url.replace('jongens', 'meisjes'): year for url, year in boy_names_urls.items()}\n",
    "urls = {**boy_names_urls, **girl_names_urls}\n",
    "\n",
    "name_time_series = defaultdict(list)\n",
    "for url, year in urls.items():\n",
    "    names = json.loads(requests.get(url).content)['data']\n",
    "    for record in names:\n",
    "        name_time_series[record[0]].append(record[1])\n",
    "\n",
    "name_time_series = dict(name_time_series)\n",
    "name_time_series = [{\n",
    "    \"title\": {\n",
    "        \"en\": f\"Number of newborns named {name}\",\n",
    "        \"nl\": f\"Aantal kinderen geboren met naam {name}\"\n",
    "    },\n",
    "    \"axis_label\": {\n",
    "        \"en\": \"Number of newborns\",\n",
    "        \"nl\": \"Aantal kinderen\"\n",
    "    },\n",
    "    \"source\": \"hhttps://www.svbkindernamen.nl/\",\n",
    "    \"values\": values\n",
    "} for name, values in name_time_series.items() if len(values) == len(boy_names_urls.keys())]\n",
    "\n",
    "name_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab20384",
   "metadata": {},
   "source": [
    "# Eredivisie\n",
    "The Eredivisie (Dutch premier league soccer) produces a gold-mine of statistics. An easily accessible set of those can be found on ererat.nl. Applying pd.read_html() directly on the url doesn't work in this case, but it does after we use the good old requests + BeautifulSoup combo to isolate the table we need. \n",
    "\n",
    "Clubs can promote in and relegated out of the Eredivisie, so not all series are complete for every club. We only include series for clubs that were in the Eredevisie during the whole period we examine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e194c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "urls = {\n",
    "    \"http://www.ererat.nl/asp/ererat_seizoen_clubstats.asp?report=5&sortitem=1&van=2014&tot=2020&clubid=-1\": {\"label\": {\"nl\": \"Aantal rode kaarten voor voetbalclub\", \"en\": \"Number of red cards for soccer club\"}, \"axis_label\": {\"en\": \"Number of red cards\", \"nl\": \"Aantal rode kaarten\"}},\n",
    "    \"http://www.ererat.nl/asp/ererat_seizoen_clubstats.asp?van=2014&tot=2020&clubid=-1&report=4\": {\"label\": {\"nl\": \"Aantal toeschouwers bij thuiswedstrijden voor voetbalclub\", \"en\": \"Number of spectators at homegames for soccer club\"}, \"axis_label\": {\"en\": \"Number of spectators\", \"nl\": \"Aantal toeschouwers\"}},\n",
    "    \"http://www.ererat.nl/asp/ererat_seizoen_clubgoals.asp?goals=1&van=2014&tot=2020&clubid=\": {\"label\": {\"nl\": \"Aantal doelpunten voor voetbalclub\", \"en\": \"Number of goals for soccer club\"}, \"axis_label\": {\"en\": \"Number of goals\", \"nl\": \"Aantal doelpunten\"}},\n",
    "    \"http://www.ererat.nl/asp/ererat_seizoen_clubgoals.asp?goals=2&van=2014&tot=2020&clubid=-1\": {\"label\": {\"nl\": \"Aantal doelpunten tegen voetbalclub\", \"en\": \"Number of goals against soccer club\"}, \"axis_label\": {\"en\": \"Number of goals\", \"nl\": \"Aantal doelpunten\"}},\n",
    "    \"http://www.ererat.nl/asp/ererat_seizoen_clubgoals.asp?goals=3&van=2014&tot=2020&clubid=-1\": {\"label\": {\"nl\": \"Doelsaldo van voetbalclub\", \"en\": \"Goal difference for soccer club\"}, \"axis_label\": {\"en\": \"Goal difference\", \"nl\": \"Doelsaldo\"}},\n",
    "    \"http://www.ererat.nl/asp/ererat_seizoen_clubwedstrijden.asp?van=2014&tot=2020&clubid=-1&report=1\": {\"label\": {\"nl\": \"Aantal wedstrijden geeindigd in winst voor voetbalclub\", \"en\": \"Number of games won by soccer club\"}, \"axis_label\": {\"en\": \"Number of games\", \"nl\": \"Aantal wedstrijden\"}},\n",
    "    \"http://www.ererat.nl/asp/ererat_seizoen_clubwedstrijden.asp?van=2014&tot=2020&clubid=-1&report=2\": {\"label\": {\"nl\": \"Aantal wedstrijden geeindigd in gelijkspel voor voetbalclub\", \"en\": \"Number of games ended in a draw for soccer club\"}, \"axis_label\": {\"en\": \"Number of games\", \"nl\": \"Aantal wedstrijden\"}},\n",
    "    \"http://www.ererat.nl/asp/ererat_seizoen_clubwedstrijden.asp?van=2014&tot=2020&clubid=-1&report=3\": {\"label\": {\"nl\": \"Aantal wedstrijden geeindigd in verlies voor voetbalclub\", \"en\": \"Number of games lost by soccer club\"}, \"axis_label\": {\"en\": \"Number of games\", \"nl\": \"Aantal wedstrijden\"}},\n",
    "    \"http://www.ererat.nl/asp/ererat_seizoen_clubstats.asp?van=2014&tot=2020&clubid=-1&report=3\": {\"label\": {\"nl\": \"Aantal unieke spelers opgesteld door voetbalclub\", \"en\": \"Number of unique players positioned by soccer club\"}, \"axis_label\": {\"en\": \"Number of players\", \"nl\": \"Aantal spelers\"}},\n",
    "}\n",
    "\n",
    "replacements = {\n",
    "    \"2014/2015\": 2014,\n",
    "    \"2015/2016\": 2015,\n",
    "    \"2016/2017\": 2016,\n",
    "    \"2017/2018\": 2017,\n",
    "    \"2018/2019\": 2018,\n",
    "    \"2019/2020\": 2019,\n",
    "    \"2020/2021\": 2020\n",
    "}\n",
    "\n",
    "eredivisie_time_series = []\n",
    "for url, labels in urls.items():\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    table = soup.find_all(\"table\", {\"class\": \"data_table\"})[0]\n",
    "    \n",
    "    df = pd.read_html(str(table))[0]\n",
    "    df = df.iloc[1:]\n",
    "    \n",
    "    for season, year in replacements.items():\n",
    "        df = df.replace(season, year)\n",
    "    \n",
    "    clubs = list(set(df[1]))\n",
    "    \n",
    "    for club in clubs:\n",
    "        values = []\n",
    "        for year in years:\n",
    "            try:\n",
    "                values.append(int(df[(df[1]==club) & (df[5]==year)][4]))\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if len(values) == 7:\n",
    "            eredivisie_time_series.append({\n",
    "                \"title\": {\n",
    "                    \"en\": f\"{labels['label']['en']} {club}\",\n",
    "                    \"nl\": f\"{labels['label']['nl']} {club}\"\n",
    "                },\n",
    "                \"axis_label\": {\n",
    "                    \"en\": labels['axis_label']['en'],\n",
    "                    \"nl\": labels['axis_label']['nl']\n",
    "                },\n",
    "                \"source\": url,\n",
    "                \"values\": values\n",
    "            })\n",
    "                \n",
    "eredivisie_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e3275",
   "metadata": {},
   "source": [
    "# ProCyclingStats\n",
    "As a cycling-enthusiast, I couldn't exclude statistics from the pro-cycling peloton from this project. Again, after isolating the table we want from a given webpage (containing the classification for an edition of a race), pd.read_html() parses that table into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "races = {\n",
    "    \"Strade Bianche\": \"https://www.procyclingstats.com/race/strade-bianche/\",\n",
    "    \"Milano-Sanremo\": \"https://www.procyclingstats.com/race/milano-sanremo/\",\n",
    "    \"Ronde van Vlaanderen\": \"https://www.procyclingstats.com/race/ronde-van-vlaanderen/\",\n",
    "    \"Liege-Bastogne-Liege\": \"https://www.procyclingstats.com/race/liege-bastogne-liege/\",\n",
    "    \"Il Lombardia\": \"https://www.procyclingstats.com/race/il-lombardia/\",\n",
    "    \"World Championship Individual Time Trial\": \"https://www.procyclingstats.com/race/world-championship-itt/\",\n",
    "    \"World Championship Road Race\": \"https://www.procyclingstats.com/race/world-championship/\",\n",
    "    \"Dutch National Championship Road Race\": \"https://www.procyclingstats.com/race/nc-netherlands/\",\n",
    "    \"Dutch National Championship Road Race\": \"https://www.procyclingstats.com/race/nc-netherlands-we/\",\n",
    "    \"Omloop Het Nieuwsblad\": \"https://www.procyclingstats.com/race/omloop-het-nieuwsblad/\",\n",
    "    \"Gent-Wevelgem\": \"https://www.procyclingstats.com/race/gent-wevelgem/\",\n",
    "    \"Kuurne - Bruxelles - Kuurne\": \"https://www.procyclingstats.com/race/kuurne-brussel-kuurne/\",\n",
    "    \"De Brabantse Pijl\": \"https://www.procyclingstats.com/race/brabantse-pijl/\",\n",
    "    \"Scheldeprijs\": \"https://www.procyclingstats.com/race/scheldeprijs/\",\n",
    "    \"Ronde van Vlaanderen\": \"https://www.procyclingstats.com/race/ronde-van-vlaanderen-we/\",\n",
    "    \"La Course\": \"https://www.procyclingstats.com/race/la-course-by-le-tour-de-france/\",\n",
    "}\n",
    "\n",
    "def remove_team_name_from_rider_name(rider_name, team_name):\n",
    "    \n",
    "    if not type(team_name) == float:\n",
    "        return rider_name.replace(team_name, '')\n",
    "    else:\n",
    "        return rider_name\n",
    "\n",
    "def switch_first_and_last_name(full_name):\n",
    "    first_name = ' '.join([word for word in full_name.split(' ') if not word.isupper()])\n",
    "    last_name = ' '.join([word for word in full_name.split(' ') if word.isupper()])\n",
    "    \n",
    "    return f\"{first_name} {last_name.title()}\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "classifications_per_rider = defaultdict(list)\n",
    "pcs_time_series = []\n",
    "\n",
    "for race, url in races.items():\n",
    "    for year in years:\n",
    "        result_url = url + str(year)         \n",
    "        response = requests.get(result_url)\n",
    "        soup = BeautifulSoup(response.content)\n",
    "        \n",
    "        print(result_url)\n",
    "        \n",
    "        table = soup.find(\"table\", {\"class\": \"results basic moblist10\"})\n",
    "        df = pd.read_html(str(table))[0]\n",
    "        \n",
    "        df['Rider'] = df.apply(lambda row: remove_team_name_from_rider_name(row['Rider'], row['Team']), axis=1)\n",
    "        df['Rider'] = df.apply(lambda row: switch_first_and_last_name(row['Rider']), axis=1)\n",
    "                \n",
    "        for rider, result in dict(zip(df['Rider'], df['Rnk'])).items():\n",
    "            try:\n",
    "                classifications_per_rider[f\"{rider} in {race}\"].append(int(result))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "classifications_per_rider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d463ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_time_series = [{\n",
    "    \"title\": {\n",
    "        \"en\": f\"Classification of cyclist {label}\",\n",
    "        \"nl\": f\"Klassering van wielrenner {label}\"\n",
    "    },\n",
    "    \"axis_label\": {\n",
    "        \"en\": \"Classification\",\n",
    "        \"nl\": \"Klassering\"\n",
    "    },\n",
    "    \"source\": [url for race, url in races.items() if race in label][0],\n",
    "    \"values\": values\n",
    "} for label, values in classifications_per_rider.items() if len(values)==len(years)]\n",
    "\n",
    "pcs_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afc5423",
   "metadata": {},
   "source": [
    "# CBS\n",
    "The Dutch central statistics bureau - unsurprisingly - offers a huge amount of data. They even offer a Python package - `cbsodata` - that can be used to access it directly from Python, into a DataFrame. For now I selected two tables: one with crime statistics and one with outcomes of surveys related to happiness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ced731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cbsodata\n",
    "\n",
    "toc = pd.DataFrame(cbsodata.get_table_list())\n",
    "\n",
    "toc_yearly = toc[toc['Frequency'].isin(['Yearly', 'Perjaar'])]\n",
    "\n",
    "toc_yearly_incl_2020 = toc_yearly[toc_yearly['Period'].str.contains('202')]\n",
    "toc_yearly_incl_2020_ned = toc_yearly_incl_2020[toc_yearly_incl_2020['Identifier'].str.contains(\"NED\")]\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "toc_yearly_incl_2020_ned[['Title', 'Identifier']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = \"83648NED\"\n",
    "\n",
    "data = pd.DataFrame(cbsodata.get_data(identifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "years_str = [str(year) for year in years]\n",
    "\n",
    "crime_types = [\n",
    "    \"17 Witwassen\",\n",
    "    \"211 Vernieling aan auto\",\n",
    "    \"23 Brandstichting / ontploffing\",\n",
    "    \"52 Rijden onder invloed\",\n",
    "    \"56 Joyriding\",\n",
    "    '61 Harddrugs',\n",
    "    '62 Softdrugs',\n",
    "]\n",
    "\n",
    "replacements = {\n",
    "    \"17 Witwassen\": \"witwassen\",\n",
    "    \"211 Vernieling aan auto\": \"vernieling aan auto\",\n",
    "    \"23 Brandstichting / ontploffing\": \"brandstichting\",\n",
    "    \"52 Rijden onder invloed\": \"rijden onder invloed\",\n",
    "    \"56 Joyriding\": \"joyriding\",\n",
    "    '61 Harddrugs': \"harddrugs\",\n",
    "    '62 Softdrugs': \"softdrugs\",\n",
    "}\n",
    "\n",
    "data_filtered = data[(data['SoortMisdrijf'].isin(crime_types)) & data['Perioden'].isin(years_str)]\n",
    "\n",
    "for original, replacement in replacements.items():\n",
    "    data_filtered['SoortMisdrijf'] = data_filtered['SoortMisdrijf'].replace(original, replacement)\n",
    "    data_filtered['RegioS'] = data_filtered['RegioS'].str.replace(original, replacement)\n",
    "\n",
    "data_filtered['SoortPerRegio'] = \"Aantal misdrijven gerelateerd aan \" + data_filtered['SoortMisdrijf'].str.lower() + \" in \" + data_filtered['RegioS']\n",
    "\n",
    "data_filtered = data_filtered[~data_filtered['RegioS'].str.contains('Niet in te delen')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48daf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up = {\n",
    "    \" (PV)\": \"\",\n",
    "    \" (O.)\": \"\",\n",
    "    \" FryslÃ¢n\": \"\",\n",
    "    \" (gemeente)\": \"\"\n",
    "}\n",
    "\n",
    "translations = {\n",
    "    \"witwassen\": \"money laundering\",\n",
    "    \"vernieling aan auto\": \"destruction/vandalism of cars\",\n",
    "    \"brandstichting\": \"arson\",\n",
    "    \"rijden onder invloed\": \"drunk driving\",\n",
    "    \"Aantal misdrijven gerelateerd aan\": \"Number of crimes related to\",\n",
    "}\n",
    "\n",
    "def clean(text):\n",
    "    for k, v in clean_up.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "def translate(text):\n",
    "    for dutch, english in translations.items():\n",
    "        text = text.replace(dutch, english)\n",
    "    return text\n",
    "\n",
    "crime_time_series = []\n",
    "\n",
    "labels = list(set(data_filtered['SoortPerRegio']))\n",
    "\n",
    "for label in labels:\n",
    "    values = list(data_filtered[data_filtered['SoortPerRegio']==label].sort_values('Perioden')['TotaalGeregistreerdeMisdrijven_1'])\n",
    "    \n",
    "    assert len(values) == len(years)\n",
    "    \n",
    "    if any([region in label for region in ['(LD)', '(RE)']]):\n",
    "        continue\n",
    "    \n",
    "    if not any([pd.isna(value) for value in values]) and all([value>100 for value in values]):\n",
    "        label = clean(label)\n",
    "        \n",
    "        if label:\n",
    "            crime_time_series.append({\n",
    "                \"title\": {\n",
    "                    \"en\": translate(label),\n",
    "                    \"nl\": label\n",
    "                },\n",
    "                \"axis_label\": {\n",
    "                    \"en\": \"Number of crimes\",\n",
    "                    \"nl\": \"Aantal misdrijven\"\n",
    "                },\n",
    "                \"source\": \"https://opendata.cbs.nl/statline/#/CBS/nl/dataset/83648NED/table?fromstatweb\",\n",
    "                \"values\": values\n",
    "            })\n",
    "\n",
    "crime_time_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01fcfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = \"82816NED\"\n",
    "\n",
    "data = pd.DataFrame(cbsodata.get_data(identifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1be16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered = data[(data['Geslacht'] == 'Totaal mannen en vrouwen') & data['Perioden'].isin(years_str) & (data['HoogstBehaaldOnderwijsniveau'] == 'Totaal') & (data['Leeftijd'] == '15 jaar of ouder') & (data['Migratieachtergrond'] == 'Totaal ')]\n",
    "\n",
    "data_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf6f5e",
   "metadata": {},
   "source": [
    "# Top 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c907f26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "FILE_NAME = os.path.join('..', 'top-2000.xlsx')\n",
    "\n",
    "df = pd.read_excel(FILE_NAME)\n",
    "\n",
    "top_2000_time_series = defaultdict(list)\n",
    "for row in df.to_dict('records'):\n",
    "    top_2000_time_series[row['Nummer']].append(row['Klassering'])\n",
    "    \n",
    "top_2000_time_series = [\n",
    "    {\n",
    "        'title': {\n",
    "            'en': f\"Position of '{title}' in the Dutch Top 2000\",\n",
    "            'nl': f\"Positie van '{title}' in de Top 2000\"\n",
    "        },\n",
    "        'axis_label': {\n",
    "            'en': 'Position',\n",
    "            'nl': 'Positie'\n",
    "        },\n",
    "        'source': 'http://www.hitsallertijden.nl/top%202000/',\n",
    "        'values': values\n",
    "    } for title, values in top_2000_time_series.items() if len(values) == 7 and min(values) < 50\n",
    "]\n",
    "\n",
    "print(top_2000_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3edec5",
   "metadata": {},
   "source": [
    "# Manual additions\n",
    "Some small datasets for which it isn't worth the effort of setting up a webscraper --> Collected manually in Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8d8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "FILE_NAME = os.path.join('..', 'manual-additions.xlsx')\n",
    "\n",
    "df = pd.read_excel(FILE_NAME)\n",
    "\n",
    "manual_addition_time_series = []\n",
    "for record in df.to_dict('records'):\n",
    "    manual_addition_time_series.append({\n",
    "        'title': {\n",
    "            'en': record['Label EN'],\n",
    "            'nl': record['Label NL']\n",
    "        },\n",
    "        'axis_label': {\n",
    "            'en': record['Label EN short'],\n",
    "            'nl': record['Label NL short']\n",
    "        },\n",
    "        'source': record['Source'],\n",
    "        'values': [record[year] for year in years]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413b53aa",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "Now let's see how closely each series correlates to all others, showing plots of those that correlate strongly (positively or negatively), while they're from different categories. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88664643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(series1, series2, years, language):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    label1 = series1['title'][language]\n",
    "    label2 = series2['title'][language]\n",
    "    y_label1 = series1['axis_label'][language]\n",
    "    y_label2 = series2['axis_label'][language]\n",
    "    plt.title(f\"{label1}\\nvs.\\n{label2}\")\n",
    "    \n",
    "    ax.plot(years, series1['values'], color=\"red\", marker=\"o\")\n",
    "    ax.set_xlabel(\"Year\",fontsize=14)\n",
    "    ax.set_ylabel(y_label1,color=\"red\")\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(years, series2['values'],color=\"blue\",marker=\"o\")\n",
    "    ax2.set_ylabel(y_label2, color=\"blue\")\n",
    "\n",
    "    plt.xticks(years)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9089a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def pearson_cor(x, y):\n",
    "    \"\"\" Copied from https://cancerdatascience.org/blog/posts/pearson-correlation/ \"\"\"\n",
    "    xv = x - x.mean(axis=0)\n",
    "    yv = y - y.mean(axis=0)\n",
    "    xvss = (xv * xv).sum(axis=0)\n",
    "    yvss = (yv * yv).sum(axis=0)\n",
    "    result = np.matmul(xv.transpose(), yv) / np.sqrt(np.outer(xvss, yvss))\n",
    "    # bound the values to -1 to 1 in the event of precision issues\n",
    "    return np.maximum(np.minimum(result, 1.0), -1.0)\n",
    "\n",
    "\n",
    "def find_max_correlation(input_series, all_series):\n",
    "    correlations = pearson_cor(input_series.transpose(), all_series.transpose())\n",
    "    max_correlation_idx = int(abs(correlations).argmax(axis=1))\n",
    "    max_correlation = correlations[0, max_correlation_idx]\n",
    "\n",
    "    return max_correlation, max_correlation_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8f44d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {\n",
    "    \"Cycling\": pcs_time_series,\n",
    "    \"Eredivisie soccer\": eredivisie_time_series,\n",
    "    \"Climate\": climate_time_series,\n",
    "    \"Movies\": imdb_time_series,\n",
    "    \"CBS\": crime_time_series,\n",
    "    \"Top 2000\": top_2000_time_series,\n",
    "    \"Manual additons\": manual_addition_time_series\n",
    "}\n",
    "language = \"en\"\n",
    "\n",
    "correlated_series = []\n",
    "used = []\n",
    "for category1, list1 in all_data.items():\n",
    "    for series1 in list1:\n",
    "        data1 = np.array(series1['values'])\n",
    "        \n",
    "        other_category_data = [series for category, category_series in all_data.items() for series in category_series if category != category1]\n",
    "        all_series = np.array([np.array(data['values']) for data in other_category_data])\n",
    "        \n",
    "        max_correlation, max_correlation_idx = find_max_correlation(data1, all_series)\n",
    "        series2 = other_category_data[max_correlation_idx]\n",
    "        \n",
    "        if abs(max_correlation) > 0.95 and [series2, series1, max_correlation] not in correlated_series:\n",
    "            correlated_series.append([series1, series2, max_correlation])\n",
    "            used.append(series1['title']['en'])\n",
    "            used.append(series2['title']['en'])\n",
    "            plot(series1, series2, years, language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7074c1",
   "metadata": {},
   "source": [
    "# Saving the dataset to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b1b4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open('../all_data.json', 'w') as outfile:\n",
    "    json.dump(all_data, outfile, indent=4)\n",
    "\n",
    "random.shuffle(correlated_series)\n",
    "print(correlated_series)\n",
    "with open('../correlated_series.json', 'w') as outfile:\n",
    "    json.dump(correlated_series, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386b4da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330cf6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
